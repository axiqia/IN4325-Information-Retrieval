\documentclass[a4paper, notitlepage]{article}
\usepackage{fullpage, listings, courier}
\usepackage{float}
\usepackage[pdftex]{graphicx}
\lstset{language=html,basicstyle=\small\ttfamily,commentstyle=\color{Gray},tabsize=4}

\begin{document}

\title{IN4325 Information Retrieval, assignment 4}
\author{Borislav Todorov (4181840) and Zmitser Zhaleznichenka (4134575)}
\date{\today}
\maketitle

\section{Exercise 20.1}

It is better to partition hosts rather than individual URLs between the nodes of a distributed crawl system as the host address usually has direct correspondence with the physical location of a host while the URLs may have nothing to do with it. 

The distribution of IP addresses is managed by a non-profit organisation IANA which assigns each country certain pools of host addresses. Regional Internet Registries in these countries then allocate the addresses from the given pool to certain organisations and providers within the country. It is known which range of IP addresses is allocated to which region and having this information one can tune a distributed crawler to process only nearby hosts to decrease latency and avoid overlaps. 

On the other hands, for URLs we cannot say in advance where the corresponding physical machine (cluster) is located for a number of reasons. First of all, there are international domains which are in use throughout the world, i.e. \lstinline{.com}, \lstinline{.org} and \lstinline{.net}. Secondly, most country code top-level domain registries allow its accredited registrars to sell the domains under their delegation to the residents of third countries. For example, it is possible to buy a domain in Belgian zone \lstinline{.be} while being an American resident and not planning to use it for mostly Belgian users and keep in Belgium (i.e. \lstinline{http://youtu.be}). Also, even if buying a domain in your national zone to host a website for the local community, it is sometimes better to keep a server abroad for the sake of savings, security and/or other reasons.

Thus, if one will partition the nodes of the distributed crawler by certain URLs, all the nodes will end up crawling the servers all over the world that will lead to decreased performance as opposed to the hosts-based partitioning. 

\section{Exercise 20.2}

The host splitter should precede the Duplicate URL Eliminator block because the survived URL submitted to the host splitter may belong to any of the partitioned crawler threads operating in parallel. The thread where the host splitter locates is aware only about the hosts within the range allocated to it. After we check the DNS record of the survived URL and make sure it should be directed to another thread, we have no cheap mechanism to check whether the URL is presented in the frontier of that thread. Thus, we send the URL to the responsible thread and the remote URL Eliminator checks whether it has a respective record in its database, which is a simple and a straightforward procedure.

However, if maintaining the local cache of URLs at the host splitter level we can decrease the amount of communication. In this case the cache will function as the local Duplicate URL Eliminator block but its functionality has to be duplicated on the remote side anyway. Also, if the bottleneck performance part of a host splitter is CPU, not network, the introduction of such a cache may lead to performance decrease so the performance measurements are needed to confirm the need in local URL cache and to tune the caching parameters.

\section{Exercise 20.4}

Section 20.4 of the book discusses an approach for encoding a row from the table of links in terms of the seven preceding rows. A 3 bit field (offset) is used in order to indicate that a row incorporates any preceding row. Using 3 bit field gives 8 ($2^3$) possible offsets(indicating 8 preceding rows). However, we need one value to indicate that a row is independent (does not incorporate any preceding rows) and therefore with 3 bit field we can specify as a prototype one of the 7 preceding rows. 

\section{Exercise 20.5}

The approach described in section 20.4 of the book significantly decreases the size of the web graph representation. However, it has a disadvantage. If you want to reconstruct an entry for a particular URL you also have to reconstruct recursively its prototype entry. This can lead to many levels of indirection. 

Lets consider a web system where a user has to go through a sequence of steps(web pages) and at each step there are links for returning to any of the previous pages. The table of links could look like that:

\emph{
\\
1:\\
2: 1\\
3: 1 2\\
4: 1 2 3\\
5: 1 2 3 4\\
}

In this case each entry will be prototyped by its predecessor. As a result, if we want to reconstruct the entry for document 5 we will also have to reconstruct all the previous entries in the table. In this example the number of levels of indirection grows linearly with the number of URLs.


\end{document}